{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Github....data upload + all models,  cats_and_dogs_classificatoin.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/PARULCHUTANIPC/parul/blob/master/Kaggle_dataset_upload_%2B_all_models%2C_cats_and_dogs_classificatoin.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iVLV8qY1LJNC"
      },
      "source": [
        "%%capture\n",
        "!pip install kaggle\n",
        "\n",
        "from zipfile import ZipFile\n",
        "import io, cv2, fnmatch, shutil, os, getpass, subprocess, random\n",
        "\n",
        "import keras\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from keras.models import Sequential, Model\n",
        "from keras.layers import Conv2D, MaxPooling2D, Activation, Dropout, Flatten, Dense\n",
        "from keras.optimizers import SGD\n",
        "from keras.callbacks import Callback, EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from time import time\n",
        "from glob import glob\n",
        "from sklearn.utils import class_weight\n",
        "from google.colab import files"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JlWMCj3CakyP"
      },
      "source": [
        "root_dir = '/content'\n",
        "download_dir = os.path.join(root_dir,'cats_dogs')\n",
        "data_dir = os.path.join(download_dir,'data')\n",
        "train_path = os.path.join(data_dir,'train')\n",
        "val_path = os.path.join(data_dir,'val')\n",
        "train_dogs_path = os.path.join(train_path,'dogs')\n",
        "train_cats_path = os.path.join(train_path,'cats')\n",
        "val_dogs_path = os.path.join(val_path,'dogs')\n",
        "val_cats_path = os.path.join(val_path,'cats')"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KSfAxmPpa6qG"
      },
      "source": [
        "data source - https://www.kaggle.com/c/dogs-vs-cats/data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vVuEZQBHjfuh",
        "outputId": "93d2f166-0a9e-427b-8c10-922563717571"
      },
      "source": [
        "!pip install kaggle"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: kaggle in /usr/local/lib/python3.7/dist-packages (1.5.12)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from kaggle) (4.41.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.7/dist-packages (from kaggle) (2020.12.5)\n",
            "Requirement already satisfied: python-slugify in /usr/local/lib/python3.7/dist-packages (from kaggle) (4.0.1)\n",
            "Requirement already satisfied: six>=1.10 in /usr/local/lib/python3.7/dist-packages (from kaggle) (1.15.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from kaggle) (2.23.0)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.7/dist-packages (from kaggle) (2.8.1)\n",
            "Requirement already satisfied: urllib3 in /usr/local/lib/python3.7/dist-packages (from kaggle) (1.24.3)\n",
            "Requirement already satisfied: text-unidecode>=1.3 in /usr/local/lib/python3.7/dist-packages (from python-slugify->kaggle) (1.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->kaggle) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->kaggle) (2.10)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LOhFt9ADjfrC"
      },
      "source": [
        "!mkdir .kaggle\n",
        "!touch .kaggle/kaggle.json"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 255
        },
        "id": "lekG8HpxV3Aq",
        "outputId": "bcd179d0-b602-4521-fd59-ba77bc355e66"
      },
      "source": [
        "os.environ['KAGGLE_CONFIG_DIR'] = root_dir\n",
        "os.chdir(root_dir)\n",
        "if 'kaggle.json' not in os.listdir(root_dir):downloaded = files.upload()\n",
        "if 'cats_dogs' in os.listdir(root_dir):shutil.rmtree(download_dir)\n",
        "os.mkdir(download_dir)\n",
        "os.chdir(download_dir)\n",
        "!kaggle competitions download -c dogs-vs-cats\n",
        "!unzip -q -o train.zip\n",
        "!unzip -q -o test1.zip"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-00374166-51db-4b48-9707-589ab429fd81\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-00374166-51db-4b48-9707-589ab429fd81\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Saving kaggle.json to kaggle.json\n",
            "Warning: Your Kaggle API key is readable by other users on this system! To fix this, you can run 'chmod 600 /content/kaggle.json'\n",
            "Warning: Looks like you're using an outdated API Version, please consider updating (server 1.5.12 / client 1.5.4)\n",
            "Downloading train.zip to /content/cats_dogs\n",
            " 98% 532M/543M [00:02<00:00, 220MB/s]\n",
            "100% 543M/543M [00:02<00:00, 204MB/s]\n",
            "Downloading test1.zip to /content/cats_dogs\n",
            " 98% 265M/271M [00:05<00:00, 49.3MB/s]\n",
            "100% 271M/271M [00:05<00:00, 55.4MB/s]\n",
            "Downloading sampleSubmission.csv to /content/cats_dogs\n",
            "  0% 0.00/86.8k [00:00<?, ?B/s]\n",
            "100% 86.8k/86.8k [00:00<00:00, 89.7MB/s]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q3wrOx42FijD",
        "outputId": "cf2d86c3-8dd6-4cc1-de5a-649ce3ce6fa3"
      },
      "source": [
        "cat_pattern = '*cat.*.jpg'\n",
        "dog_pattern = '*dog.*.jpg'\n",
        "\n",
        "images = glob('/content/cats_dogs/train/*.jpg', recursive=True)\n",
        "cats = fnmatch.filter(images,cat_pattern)\n",
        "dogs = fnmatch.filter(images,dog_pattern)\n",
        "\n",
        "if 'data' not in os.listdir(download_dir):os.mkdir(data_dir)\n",
        "if 'train' not in os.listdir(data_dir):os.mkdir(train_path)\n",
        "if 'dogs' not in os.listdir(train_path):os.mkdir(train_dogs_path)\n",
        "if 'cats' not in os.listdir(train_path):os.mkdir(train_cats_path)\n",
        "  \n",
        "if 'val' not in os.listdir(data_dir):os.mkdir(val_path)\n",
        "if 'dogs' not in os.listdir(val_path):os.mkdir(val_dogs_path)\n",
        "if 'cats' not in os.listdir(val_path):os.mkdir(val_cats_path)\n",
        "\n",
        "for file in cats: shutil.copy2(file, train_cats_path)\n",
        "for file in dogs: shutil.copy2(file, train_dogs_path)\n",
        "  \n",
        "  \n",
        "# split train date into train and validation\n",
        "train_len = len(os.listdir(train_dogs_path))\n",
        "val_len = train_len * 0.3\n",
        "val_dogs = random.sample(os.listdir(train_dogs_path),int(val_len))\n",
        "val_cats = random.sample(os.listdir(train_cats_path),int(val_len))\n",
        "\n",
        "for file in val_dogs:\n",
        "  try: shutil.move(os.path.join(train_dogs_path,file), val_dogs_path)\n",
        "  except: pass\n",
        "for file in val_cats:\n",
        "  try: shutil.move(os.path.join(train_cats_path,file), val_cats_path)\n",
        "  except: pass\n",
        "  \n",
        "print(len(os.listdir(train_cats_path)))\n",
        "print(len(os.listdir(val_cats_path)))\n",
        "\n",
        "print(len(os.listdir(train_dogs_path)))\n",
        "print(len(os.listdir(val_dogs_path)))\n",
        "\n",
        "print('total train samples ', len(os.listdir(train_cats_path)) + len(os.listdir(train_dogs_path)))\n",
        "print('total train samples ', len(os.listdir(val_cats_path)) + len(os.listdir(val_dogs_path)))"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "8750\n",
            "3750\n",
            "8750\n",
            "3750\n",
            "total train samples  17500\n",
            "total train samples  7500\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zPtRx1P_vCBF"
      },
      "source": [
        "#CNN classifier"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M8bbF3egoK2g",
        "outputId": "3f10d024-a010-4091-a4fd-ce02a6fc721f"
      },
      "source": [
        "train_datagen = ImageDataGenerator(\n",
        "        rescale=1./255,\n",
        "        shear_range=0.2,\n",
        "        zoom_range=0.2,\n",
        "        horizontal_flip=True)\n",
        "val_datagen = ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "batch_size=64\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "        '/content/cats_dogs/data/train',  # this is the target directory\n",
        "        target_size=(150, 150),  # all images will be resized to 150x150\n",
        "        batch_size=batch_size,\n",
        "        class_mode='binary') \n",
        "\n",
        "validation_generator = val_datagen.flow_from_directory(\n",
        "        '/content/cats_dogs/data/val',\n",
        "        target_size=(150, 150),\n",
        "        batch_size=batch_size,\n",
        "        class_mode='binary')\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Conv2D(64, (3, 3), input_shape=( 150, 150, 3)))\n",
        "model.add(Activation('relu'))\n",
        "model.add(MaxPooling2D(pool_size=(2,2)))\n",
        "\n",
        "model.add(Conv2D(64, (3, 3)))\n",
        "model.add(Activation('relu'))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "\n",
        "model.add(Conv2D(64, (3, 3)))\n",
        "model.add(Activation('relu'))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "\n",
        "\n",
        "model.add(Conv2D(64, (3, 3)))\n",
        "model.add(Activation('relu'))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "\n",
        "model.add(Flatten()) \n",
        "model.add(Dense(64))\n",
        "model.add(Activation('relu'))\n",
        "\n",
        "model.add(Dense(1))\n",
        "model.add(Activation('sigmoid'))\n",
        "\n",
        "model.compile(loss='binary_crossentropy',\n",
        "              optimizer='rmsprop',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "start_time = time()\n",
        "model.fit_generator(\n",
        "        train_generator,\n",
        "        #steps_per_epoch=18631 // batch_size,\n",
        "        epochs=40,\n",
        "        validation_data=validation_generator,\n",
        "        #validation_steps=10119 // batch_size\n",
        "        )\n",
        "model.save_weights('first_try.h5')\n",
        "\n",
        "print('time taken ',time()-start_time)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 17500 images belonging to 2 classes.\n",
            "Found 7500 images belonging to 2 classes.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/training.py:1844: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
            "  warnings.warn('`Model.fit_generator` is deprecated and '\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/40\n",
            "274/274 [==============================] - 161s 585ms/step - loss: 0.7043 - accuracy: 0.5398 - val_loss: 0.5798 - val_accuracy: 0.7061\n",
            "Epoch 2/40\n",
            "274/274 [==============================] - 160s 585ms/step - loss: 0.5922 - accuracy: 0.6862 - val_loss: 0.5046 - val_accuracy: 0.7580\n",
            "Epoch 3/40\n",
            "274/274 [==============================] - 160s 583ms/step - loss: 0.5179 - accuracy: 0.7439 - val_loss: 0.4308 - val_accuracy: 0.8043\n",
            "Epoch 4/40\n",
            "274/274 [==============================] - 159s 582ms/step - loss: 0.4645 - accuracy: 0.7743 - val_loss: 0.4008 - val_accuracy: 0.8263\n",
            "Epoch 5/40\n",
            "274/274 [==============================] - 159s 581ms/step - loss: 0.4166 - accuracy: 0.8058 - val_loss: 0.3956 - val_accuracy: 0.8299\n",
            "Epoch 6/40\n",
            "274/274 [==============================] - 159s 581ms/step - loss: 0.3848 - accuracy: 0.8222 - val_loss: 0.3593 - val_accuracy: 0.8425\n",
            "Epoch 7/40\n",
            "274/274 [==============================] - 158s 577ms/step - loss: 0.3605 - accuracy: 0.8373 - val_loss: 0.3268 - val_accuracy: 0.8647\n",
            "Epoch 8/40\n",
            "274/274 [==============================] - 159s 579ms/step - loss: 0.3314 - accuracy: 0.8540 - val_loss: 0.3832 - val_accuracy: 0.8479\n",
            "Epoch 9/40\n",
            "274/274 [==============================] - 159s 580ms/step - loss: 0.3056 - accuracy: 0.8661 - val_loss: 0.3106 - val_accuracy: 0.8668\n",
            "Epoch 10/40\n",
            "274/274 [==============================] - 160s 584ms/step - loss: 0.2889 - accuracy: 0.8765 - val_loss: 0.2606 - val_accuracy: 0.8927\n",
            "Epoch 11/40\n",
            "274/274 [==============================] - 159s 580ms/step - loss: 0.2622 - accuracy: 0.8865 - val_loss: 0.2808 - val_accuracy: 0.8947\n",
            "Epoch 12/40\n",
            "274/274 [==============================] - 159s 580ms/step - loss: 0.2569 - accuracy: 0.8898 - val_loss: 0.2605 - val_accuracy: 0.8941\n",
            "Epoch 13/40\n",
            "274/274 [==============================] - 159s 579ms/step - loss: 0.2416 - accuracy: 0.9001 - val_loss: 0.2495 - val_accuracy: 0.9065\n",
            "Epoch 14/40\n",
            "274/274 [==============================] - 159s 580ms/step - loss: 0.2206 - accuracy: 0.9078 - val_loss: 0.2320 - val_accuracy: 0.9096\n",
            "Epoch 15/40\n",
            "274/274 [==============================] - 159s 580ms/step - loss: 0.2120 - accuracy: 0.9105 - val_loss: 0.2104 - val_accuracy: 0.9141\n",
            "Epoch 16/40\n",
            "274/274 [==============================] - 159s 581ms/step - loss: 0.2039 - accuracy: 0.9195 - val_loss: 0.2772 - val_accuracy: 0.8872\n",
            "Epoch 17/40\n",
            "274/274 [==============================] - 159s 581ms/step - loss: 0.2025 - accuracy: 0.9152 - val_loss: 0.2242 - val_accuracy: 0.9180\n",
            "Epoch 18/40\n",
            "274/274 [==============================] - 158s 578ms/step - loss: 0.1995 - accuracy: 0.9195 - val_loss: 0.2648 - val_accuracy: 0.8900\n",
            "Epoch 19/40\n",
            "274/274 [==============================] - 159s 580ms/step - loss: 0.1899 - accuracy: 0.9233 - val_loss: 0.2292 - val_accuracy: 0.9199\n",
            "Epoch 20/40\n",
            "274/274 [==============================] - 158s 576ms/step - loss: 0.1809 - accuracy: 0.9281 - val_loss: 0.2337 - val_accuracy: 0.9077\n",
            "Epoch 21/40\n",
            "274/274 [==============================] - 159s 582ms/step - loss: 0.1749 - accuracy: 0.9297 - val_loss: 0.1951 - val_accuracy: 0.9288\n",
            "Epoch 22/40\n",
            "274/274 [==============================] - 160s 585ms/step - loss: 0.1640 - accuracy: 0.9320 - val_loss: 0.2061 - val_accuracy: 0.9243\n",
            "Epoch 23/40\n",
            "274/274 [==============================] - 159s 580ms/step - loss: 0.1654 - accuracy: 0.9333 - val_loss: 0.2455 - val_accuracy: 0.9072\n",
            "Epoch 24/40\n",
            "274/274 [==============================] - 160s 585ms/step - loss: 0.1614 - accuracy: 0.9389 - val_loss: 0.1942 - val_accuracy: 0.9264\n",
            "Epoch 25/40\n",
            "274/274 [==============================] - 163s 595ms/step - loss: 0.1641 - accuracy: 0.9356 - val_loss: 0.1976 - val_accuracy: 0.9299\n",
            "Epoch 26/40\n",
            "274/274 [==============================] - 161s 589ms/step - loss: 0.1611 - accuracy: 0.9417 - val_loss: 0.3300 - val_accuracy: 0.9017\n",
            "Epoch 27/40\n",
            "274/274 [==============================] - 159s 581ms/step - loss: 0.1664 - accuracy: 0.9370 - val_loss: 0.2534 - val_accuracy: 0.9213\n",
            "Epoch 28/40\n",
            "274/274 [==============================] - 159s 581ms/step - loss: 0.1553 - accuracy: 0.9407 - val_loss: 0.2513 - val_accuracy: 0.8996\n",
            "Epoch 29/40\n",
            "274/274 [==============================] - 160s 583ms/step - loss: 0.1533 - accuracy: 0.9378 - val_loss: 0.2494 - val_accuracy: 0.9137\n",
            "Epoch 30/40\n",
            "274/274 [==============================] - 160s 585ms/step - loss: 0.1557 - accuracy: 0.9403 - val_loss: 0.2209 - val_accuracy: 0.9215\n",
            "Epoch 31/40\n",
            "274/274 [==============================] - 160s 583ms/step - loss: 0.1565 - accuracy: 0.9421 - val_loss: 0.2169 - val_accuracy: 0.9195\n",
            "Epoch 32/40\n",
            "274/274 [==============================] - 161s 588ms/step - loss: 0.1573 - accuracy: 0.9388 - val_loss: 0.2203 - val_accuracy: 0.9245\n",
            "Epoch 33/40\n",
            "274/274 [==============================] - 161s 589ms/step - loss: 0.1475 - accuracy: 0.9448 - val_loss: 0.2557 - val_accuracy: 0.9289\n",
            "Epoch 34/40\n",
            "274/274 [==============================] - 161s 588ms/step - loss: 0.1596 - accuracy: 0.9411 - val_loss: 0.2080 - val_accuracy: 0.9276\n",
            "Epoch 35/40\n",
            "274/274 [==============================] - 161s 586ms/step - loss: 0.1465 - accuracy: 0.9423 - val_loss: 0.2455 - val_accuracy: 0.9091\n",
            "Epoch 36/40\n",
            "274/274 [==============================] - 161s 587ms/step - loss: 0.1538 - accuracy: 0.9417 - val_loss: 0.2235 - val_accuracy: 0.9291\n",
            "Epoch 37/40\n",
            "274/274 [==============================] - 162s 590ms/step - loss: 0.1502 - accuracy: 0.9434 - val_loss: 0.2199 - val_accuracy: 0.9333\n",
            "Epoch 38/40\n",
            "274/274 [==============================] - 160s 585ms/step - loss: 0.1438 - accuracy: 0.9456 - val_loss: 0.3967 - val_accuracy: 0.9048\n",
            "Epoch 39/40\n",
            "274/274 [==============================] - 161s 587ms/step - loss: 0.1600 - accuracy: 0.9373 - val_loss: 0.2960 - val_accuracy: 0.9171\n",
            "Epoch 40/40\n",
            "274/274 [==============================] - 161s 589ms/step - loss: 0.1554 - accuracy: 0.9417 - val_loss: 0.1891 - val_accuracy: 0.9245\n",
            "time taken  6392.624856233597\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qx21VUQAQz4z"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V9zA0ChBu75g"
      },
      "source": [
        "#Transfer learning"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oms7_s2lvVVJ"
      },
      "source": [
        "#BASE_MODEL = 'VGG16'\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "def transfer_learning(BASE_MODEL):\n",
        "  if BASE_MODEL=='VGG16':\n",
        "      from keras.applications.vgg16 import VGG16 as PTModel, preprocess_input\n",
        "  elif BASE_MODEL=='vgg19':\n",
        "      from keras.applications.vgg19 import VGG19 as PTModel, preprocess_input\n",
        "  elif BASE_MODEL=='ResNet50':\n",
        "      from keras.applications.resnet50 import ResNet50 as PTModel, preprocess_input\n",
        "  elif BASE_MODEL=='InceptionV3':\n",
        "      from keras.applications.inception_v3 import InceptionV3 as PTModel, preprocess_input\n",
        "  elif BASE_MODEL=='Xception':\n",
        "      from keras.applications.xception import Xception as PTModel, preprocess_input\n",
        "  elif BASE_MODEL=='DenseNet169': \n",
        "      from keras.applications.densenet import DenseNet169 as PTModel, preprocess_input\n",
        "  elif BASE_MODEL=='DenseNet121':\n",
        "      from keras.applications.densenet import DenseNet121 as PTModel, preprocess_input\n",
        "  else:\n",
        "      raise ValueError('Unknown model: {}'.format(BASE_MODEL))\n",
        "  \n",
        "  import keras\n",
        "  keras.backend.set_learning_phase(1)\n",
        "  \n",
        "  check_point_name = BASE_MODEL + '.model'\n",
        "  model_weights = BASE_MODEL + '.h5'\n",
        "\n",
        "  train_datagen = ImageDataGenerator(\n",
        "          rescale=1./255,\n",
        "          shear_range=0.2,\n",
        "          zoom_range=0.2,\n",
        "          horizontal_flip=True,\n",
        "          preprocessing_function = preprocess_input)\n",
        "\n",
        "  val_datagen = ImageDataGenerator(rescale=1./255,\n",
        "                                  preprocessing_function = preprocess_input)\n",
        "\n",
        "  batch_size=32\n",
        "  train_generator = train_datagen.flow_from_directory(\n",
        "          '/content/cats_dogs/data/train',  # this is the target directory\n",
        "          target_size=(250, 250),  # all images will be resized to 150x150\n",
        "          batch_size=batch_size,\n",
        "          class_mode='binary') \n",
        "\n",
        "  validation_generator = val_datagen.flow_from_directory(\n",
        "          '/content/cats_dogs/data/val',\n",
        "          target_size=(250, 250),\n",
        "          batch_size=batch_size,\n",
        "          class_mode='binary')    \n",
        "\n",
        "  img_rows, img_cols, img_channel = 250, 250, 3\n",
        "  base_model = PTModel(weights='imagenet'\n",
        "                     ,include_top=False, input_shape=(img_rows, img_cols, img_channel), classes = 2)\n",
        "\n",
        "  add_model = Sequential()\n",
        "  add_model.add(Flatten(input_shape=base_model.output_shape[1:]))\n",
        "  add_model.add(Dense(64, activation='relu'))\n",
        "  add_model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "  model = Model(inputs=base_model.input, outputs=add_model(base_model.output))\n",
        "\n",
        "\n",
        "  for layer in base_model.layers:\n",
        "      layer.trainable = False\n",
        "\n",
        "      if layer.name.startswith('bn'):\n",
        "          layer.call(layer.input, training=False)\n",
        "\n",
        "\n",
        "  model.compile(loss='binary_crossentropy', optimizer=SGD(lr=1e-4, momentum=0.9),\n",
        "                metrics=['accuracy'])\n",
        "  start_time = time()\n",
        "  model.fit_generator(\n",
        "          train_generator,\n",
        "          epochs=16,\n",
        "          validation_data=validation_generator,\n",
        "          #class_weight = class_weights,\n",
        "          callbacks=[ModelCheckpoint(check_point_name, monitor='val_acc', save_best_only=True)])\n",
        "  model.save_weights(model_weights)\n",
        "\n",
        "  print('time taken ',time()-start_time)"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kf1P8MRCvXeW",
        "outputId": "f62cd669-1b37-45e0-f0c6-58c5b2cfadb3"
      },
      "source": [
        "transfer_learning('VGG16')"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/backend.py:434: UserWarning: `tf.keras.backend.set_learning_phase` is deprecated and will be removed after 2020-10-11. To update it, simply pass a True/False value to the `training` argument of the `__call__` method of your layer or model.\n",
            "  warnings.warn('`tf.keras.backend.set_learning_phase` is deprecated and '\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Found 17500 images belonging to 2 classes.\n",
            "Found 7500 images belonging to 2 classes.\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/vgg16/vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "58892288/58889256 [==============================] - 1s 0us/step\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/training.py:1844: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
            "  warnings.warn('`Model.fit_generator` is deprecated and '\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/16\n",
            "547/547 [==============================] - 382s 686ms/step - loss: 0.5134 - accuracy: 0.7450 - val_loss: 0.2836 - val_accuracy: 0.8903\n",
            "Epoch 2/16\n",
            "547/547 [==============================] - 369s 674ms/step - loss: 0.2856 - accuracy: 0.8861 - val_loss: 0.2221 - val_accuracy: 0.9137\n",
            "Epoch 3/16\n",
            "547/547 [==============================] - 368s 673ms/step - loss: 0.2358 - accuracy: 0.9093 - val_loss: 0.2043 - val_accuracy: 0.9189\n",
            "Epoch 4/16\n",
            "547/547 [==============================] - 366s 670ms/step - loss: 0.2152 - accuracy: 0.9163 - val_loss: 0.2001 - val_accuracy: 0.9192\n",
            "Epoch 5/16\n",
            "547/547 [==============================] - 366s 668ms/step - loss: 0.2038 - accuracy: 0.9216 - val_loss: 0.1992 - val_accuracy: 0.9112\n",
            "Epoch 6/16\n",
            "547/547 [==============================] - 367s 671ms/step - loss: 0.1880 - accuracy: 0.9271 - val_loss: 0.1754 - val_accuracy: 0.9301\n",
            "Epoch 7/16\n",
            "547/547 [==============================] - 367s 671ms/step - loss: 0.1793 - accuracy: 0.9295 - val_loss: 0.1663 - val_accuracy: 0.9313\n",
            "Epoch 8/16\n",
            "547/547 [==============================] - 367s 671ms/step - loss: 0.1734 - accuracy: 0.9304 - val_loss: 0.1611 - val_accuracy: 0.9347\n",
            "Epoch 9/16\n",
            "547/547 [==============================] - 367s 670ms/step - loss: 0.1671 - accuracy: 0.9312 - val_loss: 0.1541 - val_accuracy: 0.9383\n",
            "Epoch 10/16\n",
            "547/547 [==============================] - 367s 671ms/step - loss: 0.1635 - accuracy: 0.9346 - val_loss: 0.1521 - val_accuracy: 0.9383\n",
            "Epoch 11/16\n",
            "547/547 [==============================] - 367s 670ms/step - loss: 0.1627 - accuracy: 0.9364 - val_loss: 0.1496 - val_accuracy: 0.9389\n",
            "Epoch 12/16\n",
            "547/547 [==============================] - 367s 671ms/step - loss: 0.1547 - accuracy: 0.9405 - val_loss: 0.1591 - val_accuracy: 0.9320\n",
            "Epoch 13/16\n",
            "547/547 [==============================] - 365s 668ms/step - loss: 0.1531 - accuracy: 0.9383 - val_loss: 0.1448 - val_accuracy: 0.9400\n",
            "Epoch 14/16\n",
            "547/547 [==============================] - 365s 667ms/step - loss: 0.1485 - accuracy: 0.9374 - val_loss: 0.1434 - val_accuracy: 0.9405\n",
            "Epoch 15/16\n",
            "547/547 [==============================] - 366s 668ms/step - loss: 0.1442 - accuracy: 0.9427 - val_loss: 0.1567 - val_accuracy: 0.9352\n",
            "Epoch 16/16\n",
            "547/547 [==============================] - 367s 671ms/step - loss: 0.1486 - accuracy: 0.9421 - val_loss: 0.1442 - val_accuracy: 0.9408\n",
            "time taken  5882.975666046143\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tOsIBkruvXig"
      },
      "source": [
        "transfer_learning('DenseNet169')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LQBx-iO0Ibjr"
      },
      "source": [
        "transfer_learning('vgg19')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jk9k3rjcIkpp"
      },
      "source": [
        "transfer_learning('InceptionV3')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lol2dij_9Kit"
      },
      "source": [
        "# running on TPU\n",
        "\n",
        "# !pip install keras==2.1\n",
        "# import tensorflow as tf\n",
        "# from tensorflow.keras.optimizers import SGD\n",
        "# from tensorflow.keras.models import Sequential, Model\n",
        "# from tensorflow.keras.layers import Conv2D, MaxPooling2D, Activation, Dropout, Flatten, Dense\n",
        "\n",
        "\n",
        "BASE_MODEL = 'VGG16'\n",
        "\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "if BASE_MODEL=='VGG16':\n",
        "    from keras.applications.vgg16 import VGG16 as PTModel, preprocess_input\n",
        "elif BASE_MODEL=='vgg19':\n",
        "    from keras.applications.vgg19 import VGG19 as PTModel, preprocess_input\n",
        "elif BASE_MODEL=='ResNet50':\n",
        "    from keras.applications.resnet50 import ResNet50 as PTModel, preprocess_input\n",
        "elif BASE_MODEL=='InceptionV3':\n",
        "    from keras.applications.inception_v3 import InceptionV3 as PTModel, preprocess_input\n",
        "elif BASE_MODEL=='Xception':\n",
        "    from keras.applications.xception import Xception as PTModel, preprocess_input\n",
        "elif BASE_MODEL=='DenseNet169': \n",
        "    from keras.applications.densenet import DenseNet169 as PTModel, preprocess_input\n",
        "elif BASE_MODEL=='DenseNet121':\n",
        "    from keras.applications.densenet import DenseNet121 as PTModel, preprocess_input\n",
        "else:\n",
        "    raise ValueError('Unknown model: {}'.format(BASE_MODEL))\n",
        "    \n",
        "    \n",
        "import keras\n",
        "keras.backend.set_learning_phase(1)\n",
        "    \n",
        "train_datagen = ImageDataGenerator(\n",
        "        rescale=1./255,\n",
        "        shear_range=0.2,\n",
        "        zoom_range=0.2,\n",
        "        horizontal_flip=True,\n",
        "        preprocessing_function = preprocess_input)\n",
        "\n",
        "val_datagen = ImageDataGenerator(rescale=1./255,\n",
        "                                preprocessing_function = preprocess_input)\n",
        "\n",
        "batch_size=32\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "        '/content/cats_dogs/data/train',  # this is the target directory\n",
        "        target_size=(200, 200),  # all images will be resized to 150x150\n",
        "        batch_size=batch_size,\n",
        "        class_mode='binary') \n",
        "\n",
        "validation_generator = val_datagen.flow_from_directory(\n",
        "        '/content/cats_dogs/data/val',\n",
        "        target_size=(200, 200),\n",
        "        batch_size=batch_size,\n",
        "        class_mode='binary')    \n",
        "\n",
        "img_rows, img_cols, img_channel = 200, 200, 3\n",
        "base_model = PTModel(weights='imagenet'\n",
        "                   ,include_top=False, input_shape=(img_rows, img_cols, img_channel), classes = 2)\n",
        "\n",
        "add_model = Sequential()\n",
        "add_model.add(Flatten(input_shape=base_model.output_shape[1:]))\n",
        "add_model.add(Dense(64, activation='relu'))\n",
        "add_model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "model = Model(inputs=base_model.input, outputs=add_model(base_model.output))\n",
        "\n",
        "\n",
        "for layer in base_model.layers:\n",
        "    layer.trainable = False\n",
        "    \n",
        "    if layer.name.startswith('bn'):\n",
        "        layer.call(layer.input, training=False)\n",
        "    \n",
        "    \n",
        "tpu_model = tf.contrib.tpu.keras_to_tpu_model(\n",
        "    model,\n",
        "    strategy=tf.contrib.tpu.TPUDistributionStrategy(\n",
        "        tf.contrib.cluster_resolver.TPUClusterResolver(tpu='grpc://' + os.environ['COLAB_TPU_ADDR'])\n",
        "    )\n",
        ")\n",
        "\n",
        "tpu_model.compile(loss='binary_crossentropy'\n",
        "              ,optimizer=SGD(lr=1e-4, momentum=0.9),\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "start_time = time()\n",
        "model.fit_generator(\n",
        "        train_generator,\n",
        "        epochs=16,\n",
        "        validation_data=validation_generator\n",
        "        #class_weight = class_weights,\n",
        "        ,callbacks=[ModelCheckpoint('Xception-transferlearning.model', monitor='val_acc', save_best_only=True)]\n",
        "        )\n",
        "model.save_weights('Xception.h5')\n",
        "\n",
        "print('time taken ',time()-start_time)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IwhhWk2j8cYe"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}