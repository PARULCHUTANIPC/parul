{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Github....data upload + all models,  cats_and_dogs_classificatoin.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/PARULCHUTANIPC/parul/blob/Paper-2/91-92-91%20Kaggle_dataset_upload_%2B_all_models%2C_cats_and_dogs_classificatoin.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iVLV8qY1LJNC"
      },
      "source": [
        "%%capture\n",
        "!pip install kaggle\n",
        "\n",
        "from zipfile import ZipFile\n",
        "import io, cv2, fnmatch, shutil, os, getpass, subprocess, random\n",
        "\n",
        "import keras\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from keras.models import Sequential, Model\n",
        "from keras.layers import Conv2D, MaxPooling2D, Activation, Dropout, Flatten, Dense\n",
        "from keras.optimizers import SGD\n",
        "from keras.callbacks import Callback, EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from time import time\n",
        "from glob import glob\n",
        "from sklearn.utils import class_weight\n",
        "from google.colab import files"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JlWMCj3CakyP"
      },
      "source": [
        "root_dir = '/content'\n",
        "download_dir = os.path.join(root_dir,'cats_dogs')\n",
        "data_dir = os.path.join(download_dir,'data')\n",
        "train_path = os.path.join(data_dir,'train')\n",
        "val_path = os.path.join(data_dir,'val')\n",
        "train_dogs_path = os.path.join(train_path,'dogs')\n",
        "train_cats_path = os.path.join(train_path,'cats')\n",
        "val_dogs_path = os.path.join(val_path,'dogs')\n",
        "val_cats_path = os.path.join(val_path,'cats')"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KSfAxmPpa6qG"
      },
      "source": [
        "data source - https://www.kaggle.com/c/dogs-vs-cats/data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vVuEZQBHjfuh",
        "outputId": "73d1fc2b-aa44-4868-d0bd-488001ed1fee"
      },
      "source": [
        "!pip install kaggle"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: kaggle in /usr/local/lib/python3.7/dist-packages (1.5.12)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from kaggle) (4.41.1)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.7/dist-packages (from kaggle) (2.8.1)\n",
            "Requirement already satisfied: six>=1.10 in /usr/local/lib/python3.7/dist-packages (from kaggle) (1.15.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.7/dist-packages (from kaggle) (2020.12.5)\n",
            "Requirement already satisfied: python-slugify in /usr/local/lib/python3.7/dist-packages (from kaggle) (4.0.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from kaggle) (2.23.0)\n",
            "Requirement already satisfied: urllib3 in /usr/local/lib/python3.7/dist-packages (from kaggle) (1.24.3)\n",
            "Requirement already satisfied: text-unidecode>=1.3 in /usr/local/lib/python3.7/dist-packages (from python-slugify->kaggle) (1.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->kaggle) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->kaggle) (2.10)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LOhFt9ADjfrC"
      },
      "source": [
        "!mkdir .kaggle\n",
        "!touch .kaggle/kaggle.json"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "Ly8gQ29weXJpZ2h0IDIwMTcgR29vZ2xlIExMQwovLwovLyBMaWNlbnNlZCB1bmRlciB0aGUgQXBhY2hlIExpY2Vuc2UsIFZlcnNpb24gMi4wICh0aGUgIkxpY2Vuc2UiKTsKLy8geW91IG1heSBub3QgdXNlIHRoaXMgZmlsZSBleGNlcHQgaW4gY29tcGxpYW5jZSB3aXRoIHRoZSBMaWNlbnNlLgovLyBZb3UgbWF5IG9idGFpbiBhIGNvcHkgb2YgdGhlIExpY2Vuc2UgYXQKLy8KLy8gICAgICBodHRwOi8vd3d3LmFwYWNoZS5vcmcvbGljZW5zZXMvTElDRU5TRS0yLjAKLy8KLy8gVW5sZXNzIHJlcXVpcmVkIGJ5IGFwcGxpY2FibGUgbGF3IG9yIGFncmVlZCB0byBpbiB3cml0aW5nLCBzb2Z0d2FyZQovLyBkaXN0cmlidXRlZCB1bmRlciB0aGUgTGljZW5zZSBpcyBkaXN0cmlidXRlZCBvbiBhbiAiQVMgSVMiIEJBU0lTLAovLyBXSVRIT1VUIFdBUlJBTlRJRVMgT1IgQ09ORElUSU9OUyBPRiBBTlkgS0lORCwgZWl0aGVyIGV4cHJlc3Mgb3IgaW1wbGllZC4KLy8gU2VlIHRoZSBMaWNlbnNlIGZvciB0aGUgc3BlY2lmaWMgbGFuZ3VhZ2UgZ292ZXJuaW5nIHBlcm1pc3Npb25zIGFuZAovLyBsaW1pdGF0aW9ucyB1bmRlciB0aGUgTGljZW5zZS4KCi8qKgogKiBAZmlsZW92ZXJ2aWV3IEhlbHBlcnMgZm9yIGdvb2dsZS5jb2xhYiBQeXRob24gbW9kdWxlLgogKi8KKGZ1bmN0aW9uKHNjb3BlKSB7CmZ1bmN0aW9uIHNwYW4odGV4dCwgc3R5bGVBdHRyaWJ1dGVzID0ge30pIHsKICBjb25zdCBlbGVtZW50ID0gZG9jdW1lbnQuY3JlYXRlRWxlbWVudCgnc3BhbicpOwogIGVsZW1lbnQudGV4dENvbnRlbnQgPSB0ZXh0OwogIGZvciAoY29uc3Qga2V5IG9mIE9iamVjdC5rZXlzKHN0eWxlQXR0cmlidXRlcykpIHsKICAgIGVsZW1lbnQuc3R5bGVba2V5XSA9IHN0eWxlQXR0cmlidXRlc1trZXldOwogIH0KICByZXR1cm4gZWxlbWVudDsKfQoKLy8gTWF4IG51bWJlciBvZiBieXRlcyB3aGljaCB3aWxsIGJlIHVwbG9hZGVkIGF0IGEgdGltZS4KY29uc3QgTUFYX1BBWUxPQURfU0laRSA9IDEwMCAqIDEwMjQ7CgpmdW5jdGlvbiBfdXBsb2FkRmlsZXMoaW5wdXRJZCwgb3V0cHV0SWQpIHsKICBjb25zdCBzdGVwcyA9IHVwbG9hZEZpbGVzU3RlcChpbnB1dElkLCBvdXRwdXRJZCk7CiAgY29uc3Qgb3V0cHV0RWxlbWVudCA9IGRvY3VtZW50LmdldEVsZW1lbnRCeUlkKG91dHB1dElkKTsKICAvLyBDYWNoZSBzdGVwcyBvbiB0aGUgb3V0cHV0RWxlbWVudCB0byBtYWtlIGl0IGF2YWlsYWJsZSBmb3IgdGhlIG5leHQgY2FsbAogIC8vIHRvIHVwbG9hZEZpbGVzQ29udGludWUgZnJvbSBQeXRob24uCiAgb3V0cHV0RWxlbWVudC5zdGVwcyA9IHN0ZXBzOwoKICByZXR1cm4gX3VwbG9hZEZpbGVzQ29udGludWUob3V0cHV0SWQpOwp9CgovLyBUaGlzIGlzIHJvdWdobHkgYW4gYXN5bmMgZ2VuZXJhdG9yIChub3Qgc3VwcG9ydGVkIGluIHRoZSBicm93c2VyIHlldCksCi8vIHdoZXJlIHRoZXJlIGFyZSBtdWx0aXBsZSBhc3luY2hyb25vdXMgc3RlcHMgYW5kIHRoZSBQeXRob24gc2lkZSBpcyBnb2luZwovLyB0byBwb2xsIGZvciBjb21wbGV0aW9uIG9mIGVhY2ggc3RlcC4KLy8gVGhpcyB1c2VzIGEgUHJvbWlzZSB0byBibG9jayB0aGUgcHl0aG9uIHNpZGUgb24gY29tcGxldGlvbiBvZiBlYWNoIHN0ZXAsCi8vIHRoZW4gcGFzc2VzIHRoZSByZXN1bHQgb2YgdGhlIHByZXZpb3VzIHN0ZXAgYXMgdGhlIGlucHV0IHRvIHRoZSBuZXh0IHN0ZXAuCmZ1bmN0aW9uIF91cGxvYWRGaWxlc0NvbnRpbnVlKG91dHB1dElkKSB7CiAgY29uc3Qgb3V0cHV0RWxlbWVudCA9IGRvY3VtZW50LmdldEVsZW1lbnRCeUlkKG91dHB1dElkKTsKICBjb25zdCBzdGVwcyA9IG91dHB1dEVsZW1lbnQuc3RlcHM7CgogIGNvbnN0IG5leHQgPSBzdGVwcy5uZXh0KG91dHB1dEVsZW1lbnQubGFzdFByb21pc2VWYWx1ZSk7CiAgcmV0dXJuIFByb21pc2UucmVzb2x2ZShuZXh0LnZhbHVlLnByb21pc2UpLnRoZW4oKHZhbHVlKSA9PiB7CiAgICAvLyBDYWNoZSB0aGUgbGFzdCBwcm9taXNlIHZhbHVlIHRvIG1ha2UgaXQgYXZhaWxhYmxlIHRvIHRoZSBuZXh0CiAgICAvLyBzdGVwIG9mIHRoZSBnZW5lcmF0b3IuCiAgICBvdXRwdXRFbGVtZW50Lmxhc3RQcm9taXNlVmFsdWUgPSB2YWx1ZTsKICAgIHJldHVybiBuZXh0LnZhbHVlLnJlc3BvbnNlOwogIH0pOwp9CgovKioKICogR2VuZXJhdG9yIGZ1bmN0aW9uIHdoaWNoIGlzIGNhbGxlZCBiZXR3ZWVuIGVhY2ggYXN5bmMgc3RlcCBvZiB0aGUgdXBsb2FkCiAqIHByb2Nlc3MuCiAqIEBwYXJhbSB7c3RyaW5nfSBpbnB1dElkIEVsZW1lbnQgSUQgb2YgdGhlIGlucHV0IGZpbGUgcGlja2VyIGVsZW1lbnQuCiAqIEBwYXJhbSB7c3RyaW5nfSBvdXRwdXRJZCBFbGVtZW50IElEIG9mIHRoZSBvdXRwdXQgZGlzcGxheS4KICogQHJldHVybiB7IUl0ZXJhYmxlPCFPYmplY3Q+fSBJdGVyYWJsZSBvZiBuZXh0IHN0ZXBzLgogKi8KZnVuY3Rpb24qIHVwbG9hZEZpbGVzU3RlcChpbnB1dElkLCBvdXRwdXRJZCkgewogIGNvbnN0IGlucHV0RWxlbWVudCA9IGRvY3VtZW50LmdldEVsZW1lbnRCeUlkKGlucHV0SWQpOwogIGlucHV0RWxlbWVudC5kaXNhYmxlZCA9IGZhbHNlOwoKICBjb25zdCBvdXRwdXRFbGVtZW50ID0gZG9jdW1lbnQuZ2V0RWxlbWVudEJ5SWQob3V0cHV0SWQpOwogIG91dHB1dEVsZW1lbnQuaW5uZXJIVE1MID0gJyc7CgogIGNvbnN0IHBpY2tlZFByb21pc2UgPSBuZXcgUHJvbWlzZSgocmVzb2x2ZSkgPT4gewogICAgaW5wdXRFbGVtZW50LmFkZEV2ZW50TGlzdGVuZXIoJ2NoYW5nZScsIChlKSA9PiB7CiAgICAgIHJlc29sdmUoZS50YXJnZXQuZmlsZXMpOwogICAgfSk7CiAgfSk7CgogIGNvbnN0IGNhbmNlbCA9IGRvY3VtZW50LmNyZWF0ZUVsZW1lbnQoJ2J1dHRvbicpOwogIGlucHV0RWxlbWVudC5wYXJlbnRFbGVtZW50LmFwcGVuZENoaWxkKGNhbmNlbCk7CiAgY2FuY2VsLnRleHRDb250ZW50ID0gJ0NhbmNlbCB1cGxvYWQnOwogIGNvbnN0IGNhbmNlbFByb21pc2UgPSBuZXcgUHJvbWlzZSgocmVzb2x2ZSkgPT4gewogICAgY2FuY2VsLm9uY2xpY2sgPSAoKSA9PiB7CiAgICAgIHJlc29sdmUobnVsbCk7CiAgICB9OwogIH0pOwoKICAvLyBXYWl0IGZvciB0aGUgdXNlciB0byBwaWNrIHRoZSBmaWxlcy4KICBjb25zdCBmaWxlcyA9IHlpZWxkIHsKICAgIHByb21pc2U6IFByb21pc2UucmFjZShbcGlja2VkUHJvbWlzZSwgY2FuY2VsUHJvbWlzZV0pLAogICAgcmVzcG9uc2U6IHsKICAgICAgYWN0aW9uOiAnc3RhcnRpbmcnLAogICAgfQogIH07CgogIGNhbmNlbC5yZW1vdmUoKTsKCiAgLy8gRGlzYWJsZSB0aGUgaW5wdXQgZWxlbWVudCBzaW5jZSBmdXJ0aGVyIHBpY2tzIGFyZSBub3QgYWxsb3dlZC4KICBpbnB1dEVsZW1lbnQuZGlzYWJsZWQgPSB0cnVlOwoKICBpZiAoIWZpbGVzKSB7CiAgICByZXR1cm4gewogICAgICByZXNwb25zZTogewogICAgICAgIGFjdGlvbjogJ2NvbXBsZXRlJywKICAgICAgfQogICAgfTsKICB9CgogIGZvciAoY29uc3QgZmlsZSBvZiBmaWxlcykgewogICAgY29uc3QgbGkgPSBkb2N1bWVudC5jcmVhdGVFbGVtZW50KCdsaScpOwogICAgbGkuYXBwZW5kKHNwYW4oZmlsZS5uYW1lLCB7Zm9udFdlaWdodDogJ2JvbGQnfSkpOwogICAgbGkuYXBwZW5kKHNwYW4oCiAgICAgICAgYCgke2ZpbGUudHlwZSB8fCAnbi9hJ30pIC0gJHtmaWxlLnNpemV9IGJ5dGVzLCBgICsKICAgICAgICBgbGFzdCBtb2RpZmllZDogJHsKICAgICAgICAgICAgZmlsZS5sYXN0TW9kaWZpZWREYXRlID8gZmlsZS5sYXN0TW9kaWZpZWREYXRlLnRvTG9jYWxlRGF0ZVN0cmluZygpIDoKICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgJ24vYSd9IC0gYCkpOwogICAgY29uc3QgcGVyY2VudCA9IHNwYW4oJzAlIGRvbmUnKTsKICAgIGxpLmFwcGVuZENoaWxkKHBlcmNlbnQpOwoKICAgIG91dHB1dEVsZW1lbnQuYXBwZW5kQ2hpbGQobGkpOwoKICAgIGNvbnN0IGZpbGVEYXRhUHJvbWlzZSA9IG5ldyBQcm9taXNlKChyZXNvbHZlKSA9PiB7CiAgICAgIGNvbnN0IHJlYWRlciA9IG5ldyBGaWxlUmVhZGVyKCk7CiAgICAgIHJlYWRlci5vbmxvYWQgPSAoZSkgPT4gewogICAgICAgIHJlc29sdmUoZS50YXJnZXQucmVzdWx0KTsKICAgICAgfTsKICAgICAgcmVhZGVyLnJlYWRBc0FycmF5QnVmZmVyKGZpbGUpOwogICAgfSk7CiAgICAvLyBXYWl0IGZvciB0aGUgZGF0YSB0byBiZSByZWFkeS4KICAgIGxldCBmaWxlRGF0YSA9IHlpZWxkIHsKICAgICAgcHJvbWlzZTogZmlsZURhdGFQcm9taXNlLAogICAgICByZXNwb25zZTogewogICAgICAgIGFjdGlvbjogJ2NvbnRpbnVlJywKICAgICAgfQogICAgfTsKCiAgICAvLyBVc2UgYSBjaHVua2VkIHNlbmRpbmcgdG8gYXZvaWQgbWVzc2FnZSBzaXplIGxpbWl0cy4gU2VlIGIvNjIxMTU2NjAuCiAgICBsZXQgcG9zaXRpb24gPSAwOwogICAgd2hpbGUgKHBvc2l0aW9uIDwgZmlsZURhdGEuYnl0ZUxlbmd0aCkgewogICAgICBjb25zdCBsZW5ndGggPSBNYXRoLm1pbihmaWxlRGF0YS5ieXRlTGVuZ3RoIC0gcG9zaXRpb24sIE1BWF9QQVlMT0FEX1NJWkUpOwogICAgICBjb25zdCBjaHVuayA9IG5ldyBVaW50OEFycmF5KGZpbGVEYXRhLCBwb3NpdGlvbiwgbGVuZ3RoKTsKICAgICAgcG9zaXRpb24gKz0gbGVuZ3RoOwoKICAgICAgY29uc3QgYmFzZTY0ID0gYnRvYShTdHJpbmcuZnJvbUNoYXJDb2RlLmFwcGx5KG51bGwsIGNodW5rKSk7CiAgICAgIHlpZWxkIHsKICAgICAgICByZXNwb25zZTogewogICAgICAgICAgYWN0aW9uOiAnYXBwZW5kJywKICAgICAgICAgIGZpbGU6IGZpbGUubmFtZSwKICAgICAgICAgIGRhdGE6IGJhc2U2NCwKICAgICAgICB9LAogICAgICB9OwogICAgICBwZXJjZW50LnRleHRDb250ZW50ID0KICAgICAgICAgIGAke01hdGgucm91bmQoKHBvc2l0aW9uIC8gZmlsZURhdGEuYnl0ZUxlbmd0aCkgKiAxMDApfSUgZG9uZWA7CiAgICB9CiAgfQoKICAvLyBBbGwgZG9uZS4KICB5aWVsZCB7CiAgICByZXNwb25zZTogewogICAgICBhY3Rpb246ICdjb21wbGV0ZScsCiAgICB9CiAgfTsKfQoKc2NvcGUuZ29vZ2xlID0gc2NvcGUuZ29vZ2xlIHx8IHt9OwpzY29wZS5nb29nbGUuY29sYWIgPSBzY29wZS5nb29nbGUuY29sYWIgfHwge307CnNjb3BlLmdvb2dsZS5jb2xhYi5fZmlsZXMgPSB7CiAgX3VwbG9hZEZpbGVzLAogIF91cGxvYWRGaWxlc0NvbnRpbnVlLAp9Owp9KShzZWxmKTsK",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 255
        },
        "id": "lekG8HpxV3Aq",
        "outputId": "4ed94cac-70bf-43e3-b0b8-01e3e4905016"
      },
      "source": [
        "os.environ['KAGGLE_CONFIG_DIR'] = root_dir\n",
        "os.chdir(root_dir)\n",
        "if 'kaggle.json' not in os.listdir(root_dir):downloaded = files.upload()\n",
        "if 'cats_dogs' in os.listdir(root_dir):shutil.rmtree(download_dir)\n",
        "os.mkdir(download_dir)\n",
        "os.chdir(download_dir)\n",
        "!kaggle competitions download -c dogs-vs-cats\n",
        "!unzip -q -o train.zip\n",
        "!unzip -q -o test1.zip"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-1db05b98-e9ea-4c70-88f5-433f3153f45e\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-1db05b98-e9ea-4c70-88f5-433f3153f45e\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Saving kaggle.json to kaggle.json\n",
            "Warning: Your Kaggle API key is readable by other users on this system! To fix this, you can run 'chmod 600 /content/kaggle.json'\n",
            "Warning: Looks like you're using an outdated API Version, please consider updating (server 1.5.12 / client 1.5.4)\n",
            "Downloading test1.zip to /content/cats_dogs\n",
            " 98% 267M/271M [00:04<00:00, 95.2MB/s]\n",
            "100% 271M/271M [00:04<00:00, 62.8MB/s]\n",
            "Downloading sampleSubmission.csv to /content/cats_dogs\n",
            "  0% 0.00/86.8k [00:00<?, ?B/s]\n",
            "100% 86.8k/86.8k [00:00<00:00, 88.7MB/s]\n",
            "Downloading train.zip to /content/cats_dogs\n",
            " 99% 537M/543M [00:09<00:00, 83.5MB/s]\n",
            "100% 543M/543M [00:09<00:00, 59.4MB/s]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q3wrOx42FijD",
        "outputId": "be439459-baa7-4529-cb89-d05a7fb0c19d"
      },
      "source": [
        "cat_pattern = '*cat.*.jpg'\n",
        "dog_pattern = '*dog.*.jpg'\n",
        "\n",
        "images = glob('/content/cats_dogs/train/*.jpg', recursive=True)\n",
        "cats = fnmatch.filter(images,cat_pattern)\n",
        "dogs = fnmatch.filter(images,dog_pattern)\n",
        "\n",
        "if 'data' not in os.listdir(download_dir):os.mkdir(data_dir)\n",
        "if 'train' not in os.listdir(data_dir):os.mkdir(train_path)\n",
        "if 'dogs' not in os.listdir(train_path):os.mkdir(train_dogs_path)\n",
        "if 'cats' not in os.listdir(train_path):os.mkdir(train_cats_path)\n",
        "  \n",
        "if 'val' not in os.listdir(data_dir):os.mkdir(val_path)\n",
        "if 'dogs' not in os.listdir(val_path):os.mkdir(val_dogs_path)\n",
        "if 'cats' not in os.listdir(val_path):os.mkdir(val_cats_path)\n",
        "\n",
        "for file in cats: shutil.copy2(file, train_cats_path)\n",
        "for file in dogs: shutil.copy2(file, train_dogs_path)\n",
        "  \n",
        "  \n",
        "# split train date into train and validation\n",
        "train_len = len(os.listdir(train_dogs_path))\n",
        "val_len = train_len * 0.3\n",
        "val_dogs = random.sample(os.listdir(train_dogs_path),int(val_len))\n",
        "val_cats = random.sample(os.listdir(train_cats_path),int(val_len))\n",
        "\n",
        "for file in val_dogs:\n",
        "  try: shutil.move(os.path.join(train_dogs_path,file), val_dogs_path)\n",
        "  except: pass\n",
        "for file in val_cats:\n",
        "  try: shutil.move(os.path.join(train_cats_path,file), val_cats_path)\n",
        "  except: pass\n",
        "  \n",
        "print(len(os.listdir(train_cats_path)))\n",
        "print(len(os.listdir(val_cats_path)))\n",
        "\n",
        "print(len(os.listdir(train_dogs_path)))\n",
        "print(len(os.listdir(val_dogs_path)))\n",
        "\n",
        "print('total train samples ', len(os.listdir(train_cats_path)) + len(os.listdir(train_dogs_path)))\n",
        "print('total train samples ', len(os.listdir(val_cats_path)) + len(os.listdir(val_dogs_path)))"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "8750\n",
            "3750\n",
            "8750\n",
            "3750\n",
            "total train samples  17500\n",
            "total train samples  7500\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zPtRx1P_vCBF"
      },
      "source": [
        "#CNN classifier"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M8bbF3egoK2g",
        "outputId": "48ee394d-1afd-4c45-d189-04fa8be8d545"
      },
      "source": [
        "train_datagen = ImageDataGenerator(\n",
        "        rescale=1./255,\n",
        "        shear_range=0.2,\n",
        "        zoom_range=0.2,\n",
        "        horizontal_flip=True)\n",
        "val_datagen = ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "batch_size=64\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "        '/content/cats_dogs/data/train',  # this is the target directory\n",
        "        target_size=(150, 150),  # all images will be resized to 150x150\n",
        "        batch_size=batch_size,\n",
        "        class_mode='binary') \n",
        "\n",
        "validation_generator = val_datagen.flow_from_directory(\n",
        "        '/content/cats_dogs/data/val',\n",
        "        target_size=(150, 150),\n",
        "        batch_size=batch_size,\n",
        "        class_mode='binary')\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Conv2D(150, (3, 3), input_shape=( 150, 150, 3)))\n",
        "model.add(Activation('relu'))\n",
        "model.add(MaxPooling2D(pool_size=(2,2)))\n",
        "\n",
        "model.add(Conv2D(200, (3, 3)))\n",
        "model.add(Activation('relu'))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "\n",
        "model.add(Conv2D(215, (3, 3)))\n",
        "model.add(Activation('relu'))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "\n",
        "\n",
        "model.add(Conv2D(190, (3, 3)))\n",
        "model.add(Activation('relu'))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "\n",
        "model.add(Flatten()) \n",
        "model.add(Dense(300))\n",
        "model.add(Activation('relu'))\n",
        "\n",
        "model.add(Dense(1))\n",
        "model.add(Activation('sigmoid'))\n",
        "\n",
        "model.compile(loss='binary_crossentropy',\n",
        "              optimizer='rmsprop',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "start_time = time()\n",
        "model.fit_generator(\n",
        "        train_generator,\n",
        "        #steps_per_epoch=18631 // batch_size,\n",
        "        epochs=40,\n",
        "        validation_data=validation_generator,\n",
        "        #validation_steps=10119 // batch_size\n",
        "        )\n",
        "model.save_weights('first_try.h5')\n",
        "\n",
        "print('time taken ',time()-start_time)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 17500 images belonging to 2 classes.\n",
            "Found 7500 images belonging to 2 classes.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/training.py:1844: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
            "  warnings.warn('`Model.fit_generator` is deprecated and '\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/40\n",
            "274/274 [==============================] - 244s 761ms/step - loss: 1.0684 - accuracy: 0.5189 - val_loss: 0.6423 - val_accuracy: 0.6435\n",
            "Epoch 2/40\n",
            "274/274 [==============================] - 205s 748ms/step - loss: 0.6556 - accuracy: 0.6378 - val_loss: 0.5441 - val_accuracy: 0.7212\n",
            "Epoch 3/40\n",
            "274/274 [==============================] - 206s 750ms/step - loss: 0.5699 - accuracy: 0.7102 - val_loss: 0.5870 - val_accuracy: 0.6757\n",
            "Epoch 4/40\n",
            "274/274 [==============================] - 206s 750ms/step - loss: 0.4837 - accuracy: 0.7667 - val_loss: 0.4269 - val_accuracy: 0.8101\n",
            "Epoch 5/40\n",
            "274/274 [==============================] - 205s 747ms/step - loss: 0.4448 - accuracy: 0.7957 - val_loss: 0.3920 - val_accuracy: 0.8293\n",
            "Epoch 6/40\n",
            "274/274 [==============================] - 206s 750ms/step - loss: 0.4031 - accuracy: 0.8202 - val_loss: 0.4185 - val_accuracy: 0.8033\n",
            "Epoch 7/40\n",
            "274/274 [==============================] - 205s 748ms/step - loss: 0.3641 - accuracy: 0.8394 - val_loss: 0.3149 - val_accuracy: 0.8677\n",
            "Epoch 8/40\n",
            "274/274 [==============================] - 204s 745ms/step - loss: 0.3301 - accuracy: 0.8605 - val_loss: 0.3048 - val_accuracy: 0.8712\n",
            "Epoch 9/40\n",
            "274/274 [==============================] - 204s 743ms/step - loss: 0.3172 - accuracy: 0.8638 - val_loss: 0.2669 - val_accuracy: 0.8933\n",
            "Epoch 10/40\n",
            "274/274 [==============================] - 203s 741ms/step - loss: 0.2824 - accuracy: 0.8793 - val_loss: 0.2574 - val_accuracy: 0.8913\n",
            "Epoch 11/40\n",
            "274/274 [==============================] - 203s 741ms/step - loss: 0.2678 - accuracy: 0.8824 - val_loss: 0.2641 - val_accuracy: 0.8929\n",
            "Epoch 12/40\n",
            "274/274 [==============================] - 203s 740ms/step - loss: 0.2527 - accuracy: 0.8995 - val_loss: 0.4380 - val_accuracy: 0.8068\n",
            "Epoch 13/40\n",
            "274/274 [==============================] - 203s 739ms/step - loss: 0.2383 - accuracy: 0.8995 - val_loss: 0.2366 - val_accuracy: 0.9053\n",
            "Epoch 14/40\n",
            "274/274 [==============================] - 203s 740ms/step - loss: 0.2380 - accuracy: 0.9038 - val_loss: 0.2479 - val_accuracy: 0.9123\n",
            "Epoch 15/40\n",
            "274/274 [==============================] - 202s 738ms/step - loss: 0.2503 - accuracy: 0.9035 - val_loss: 0.2685 - val_accuracy: 0.9052\n",
            "Epoch 16/40\n",
            "274/274 [==============================] - 202s 738ms/step - loss: 0.2389 - accuracy: 0.9030 - val_loss: 0.2435 - val_accuracy: 0.9169\n",
            "Epoch 17/40\n",
            "274/274 [==============================] - 203s 739ms/step - loss: 0.2246 - accuracy: 0.9088 - val_loss: 0.2283 - val_accuracy: 0.9139\n",
            "Epoch 18/40\n",
            "274/274 [==============================] - 202s 737ms/step - loss: 0.2257 - accuracy: 0.9119 - val_loss: 0.2466 - val_accuracy: 0.9117\n",
            "Epoch 19/40\n",
            "274/274 [==============================] - 202s 738ms/step - loss: 0.2177 - accuracy: 0.9134 - val_loss: 0.2569 - val_accuracy: 0.8960\n",
            "Epoch 20/40\n",
            "274/274 [==============================] - 202s 738ms/step - loss: 0.2120 - accuracy: 0.9158 - val_loss: 0.2249 - val_accuracy: 0.9267\n",
            "Epoch 21/40\n",
            "274/274 [==============================] - 203s 739ms/step - loss: 0.2242 - accuracy: 0.9136 - val_loss: 0.1988 - val_accuracy: 0.9196\n",
            "Epoch 22/40\n",
            "274/274 [==============================] - 203s 740ms/step - loss: 0.2047 - accuracy: 0.9203 - val_loss: 0.2444 - val_accuracy: 0.9027\n",
            "Epoch 23/40\n",
            "274/274 [==============================] - 202s 738ms/step - loss: 0.2037 - accuracy: 0.9236 - val_loss: 0.1840 - val_accuracy: 0.9313\n",
            "Epoch 24/40\n",
            "274/274 [==============================] - 202s 738ms/step - loss: 0.2038 - accuracy: 0.9200 - val_loss: 0.2293 - val_accuracy: 0.9183\n",
            "Epoch 25/40\n",
            "274/274 [==============================] - 202s 737ms/step - loss: 0.1960 - accuracy: 0.9242 - val_loss: 0.2508 - val_accuracy: 0.9156\n",
            "Epoch 26/40\n",
            "274/274 [==============================] - 201s 733ms/step - loss: 0.2075 - accuracy: 0.9181 - val_loss: 0.2011 - val_accuracy: 0.9311\n",
            "Epoch 27/40\n",
            "274/274 [==============================] - 202s 735ms/step - loss: 0.2148 - accuracy: 0.9183 - val_loss: 0.2064 - val_accuracy: 0.9229\n",
            "Epoch 28/40\n",
            "274/274 [==============================] - 202s 737ms/step - loss: 0.2284 - accuracy: 0.9141 - val_loss: 0.3331 - val_accuracy: 0.8924\n",
            "Epoch 29/40\n",
            "274/274 [==============================] - 202s 737ms/step - loss: 0.2229 - accuracy: 0.9193 - val_loss: 0.3051 - val_accuracy: 0.9091\n",
            "Epoch 30/40\n",
            "274/274 [==============================] - 202s 736ms/step - loss: 0.2283 - accuracy: 0.9111 - val_loss: 0.2664 - val_accuracy: 0.9001\n",
            "Epoch 31/40\n",
            "274/274 [==============================] - 202s 736ms/step - loss: 0.2086 - accuracy: 0.9189 - val_loss: 0.2095 - val_accuracy: 0.9269\n",
            "Epoch 32/40\n",
            "274/274 [==============================] - ETA: 0s - loss: 0.2138 - accuracy: 0.9233Epoch 33/40\n",
            "274/274 [==============================] - 201s 734ms/step - loss: 0.2275 - accuracy: 0.9136 - val_loss: 0.2939 - val_accuracy: 0.9152\n",
            "Epoch 34/40\n",
            "274/274 [==============================] - ETA: 0s - loss: 0.2066 - accuracy: 0.9171Epoch 35/40\n",
            "274/274 [==============================] - 201s 733ms/step - loss: 0.1968 - accuracy: 0.9224 - val_loss: 0.2625 - val_accuracy: 0.9107\n",
            "Epoch 36/40\n",
            "274/274 [==============================] - ETA: 0s - loss: 0.2109 - accuracy: 0.9196Epoch 37/40\n",
            "274/274 [==============================] - 202s 736ms/step - loss: 0.2055 - accuracy: 0.9214 - val_loss: 0.2248 - val_accuracy: 0.9219\n",
            "Epoch 38/40\n",
            "274/274 [==============================] - 202s 736ms/step - loss: 0.2024 - accuracy: 0.9213 - val_loss: 0.1965 - val_accuracy: 0.9387\n",
            "Epoch 39/40\n",
            "274/274 [==============================] - 201s 734ms/step - loss: 0.2179 - accuracy: 0.9181 - val_loss: 0.2258 - val_accuracy: 0.9344\n",
            "Epoch 40/40\n",
            "274/274 [==============================] - 202s 736ms/step - loss: 0.2323 - accuracy: 0.9105 - val_loss: 0.2059 - val_accuracy: 0.9216\n",
            "time taken  8151.947915792465\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qx21VUQAQz4z"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V9zA0ChBu75g"
      },
      "source": [
        "#Transfer learning"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oms7_s2lvVVJ"
      },
      "source": [
        "#BASE_MODEL = 'VGG16'\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "def transfer_learning(BASE_MODEL):\n",
        "  if BASE_MODEL=='VGG16':\n",
        "      from keras.applications.vgg16 import VGG16 as PTModel, preprocess_input\n",
        "  elif BASE_MODEL=='vgg19':\n",
        "      from keras.applications.vgg19 import VGG19 as PTModel, preprocess_input\n",
        "  elif BASE_MODEL=='ResNet50':\n",
        "      from keras.applications.resnet50 import ResNet50 as PTModel, preprocess_input\n",
        "  elif BASE_MODEL=='InceptionV3':\n",
        "      from keras.applications.inception_v3 import InceptionV3 as PTModel, preprocess_input\n",
        "  elif BASE_MODEL=='Xception':\n",
        "      from keras.applications.xception import Xception as PTModel, preprocess_input\n",
        "  elif BASE_MODEL=='DenseNet169': \n",
        "      from keras.applications.densenet import DenseNet169 as PTModel, preprocess_input\n",
        "  elif BASE_MODEL=='DenseNet121':\n",
        "      from keras.applications.densenet import DenseNet121 as PTModel, preprocess_input\n",
        "  else:\n",
        "      raise ValueError('Unknown model: {}'.format(BASE_MODEL))\n",
        "  \n",
        "  import keras\n",
        "  keras.backend.set_learning_phase(1)\n",
        "  \n",
        "  check_point_name = BASE_MODEL + '.model'\n",
        "  model_weights = BASE_MODEL + '.h5'\n",
        "\n",
        "  train_datagen = ImageDataGenerator(\n",
        "          rescale=1./255,\n",
        "          shear_range=0.2,\n",
        "          zoom_range=0.2,\n",
        "          horizontal_flip=True,\n",
        "          preprocessing_function = preprocess_input)\n",
        "\n",
        "  val_datagen = ImageDataGenerator(rescale=1./255,\n",
        "                                  preprocessing_function = preprocess_input)\n",
        "\n",
        "  batch_size=32\n",
        "  train_generator = train_datagen.flow_from_directory(\n",
        "          '/content/cats_dogs/data/train',  # this is the target directory\n",
        "          target_size=(250, 250),  # all images will be resized to 150x150\n",
        "          batch_size=batch_size,\n",
        "          class_mode='binary') \n",
        "\n",
        "  validation_generator = val_datagen.flow_from_directory(\n",
        "          '/content/cats_dogs/data/val',\n",
        "          target_size=(250, 250),\n",
        "          batch_size=batch_size,\n",
        "          class_mode='binary')    \n",
        "\n",
        "  img_rows, img_cols, img_channel = 250, 250, 3\n",
        "  base_model = PTModel(weights='imagenet'\n",
        "                     ,include_top=False, input_shape=(img_rows, img_cols, img_channel), classes = 2)\n",
        "\n",
        "  add_model = Sequential()\n",
        "  add_model.add(Flatten(input_shape=base_model.output_shape[1:]))\n",
        "  add_model.add(Dense(64, activation='relu'))\n",
        "  add_model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "  model = Model(inputs=base_model.input, outputs=add_model(base_model.output))\n",
        "\n",
        "\n",
        "  for layer in base_model.layers:\n",
        "      layer.trainable = False\n",
        "\n",
        "      if layer.name.startswith('bn'):\n",
        "          layer.call(layer.input, training=False)\n",
        "\n",
        "\n",
        "  model.compile(loss='binary_crossentropy', optimizer=SGD(lr=1e-4, momentum=0.9),\n",
        "                metrics=['accuracy'])\n",
        "  start_time = time()\n",
        "  model.fit_generator(\n",
        "          train_generator,\n",
        "          epochs=16,\n",
        "          validation_data=validation_generator,\n",
        "          #class_weight = class_weights,\n",
        "          callbacks=[ModelCheckpoint(check_point_name, monitor='val_acc', save_best_only=True)])\n",
        "  model.save_weights(model_weights)\n",
        "\n",
        "  print('time taken ',time()-start_time)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kf1P8MRCvXeW",
        "outputId": "f62cd669-1b37-45e0-f0c6-58c5b2cfadb3"
      },
      "source": [
        "transfer_learning('VGG16')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/backend.py:434: UserWarning: `tf.keras.backend.set_learning_phase` is deprecated and will be removed after 2020-10-11. To update it, simply pass a True/False value to the `training` argument of the `__call__` method of your layer or model.\n",
            "  warnings.warn('`tf.keras.backend.set_learning_phase` is deprecated and '\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Found 17500 images belonging to 2 classes.\n",
            "Found 7500 images belonging to 2 classes.\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/vgg16/vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "58892288/58889256 [==============================] - 1s 0us/step\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/training.py:1844: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
            "  warnings.warn('`Model.fit_generator` is deprecated and '\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/16\n",
            "547/547 [==============================] - 382s 686ms/step - loss: 0.5134 - accuracy: 0.7450 - val_loss: 0.2836 - val_accuracy: 0.8903\n",
            "Epoch 2/16\n",
            "547/547 [==============================] - 369s 674ms/step - loss: 0.2856 - accuracy: 0.8861 - val_loss: 0.2221 - val_accuracy: 0.9137\n",
            "Epoch 3/16\n",
            "547/547 [==============================] - 368s 673ms/step - loss: 0.2358 - accuracy: 0.9093 - val_loss: 0.2043 - val_accuracy: 0.9189\n",
            "Epoch 4/16\n",
            "547/547 [==============================] - 366s 670ms/step - loss: 0.2152 - accuracy: 0.9163 - val_loss: 0.2001 - val_accuracy: 0.9192\n",
            "Epoch 5/16\n",
            "547/547 [==============================] - 366s 668ms/step - loss: 0.2038 - accuracy: 0.9216 - val_loss: 0.1992 - val_accuracy: 0.9112\n",
            "Epoch 6/16\n",
            "547/547 [==============================] - 367s 671ms/step - loss: 0.1880 - accuracy: 0.9271 - val_loss: 0.1754 - val_accuracy: 0.9301\n",
            "Epoch 7/16\n",
            "547/547 [==============================] - 367s 671ms/step - loss: 0.1793 - accuracy: 0.9295 - val_loss: 0.1663 - val_accuracy: 0.9313\n",
            "Epoch 8/16\n",
            "547/547 [==============================] - 367s 671ms/step - loss: 0.1734 - accuracy: 0.9304 - val_loss: 0.1611 - val_accuracy: 0.9347\n",
            "Epoch 9/16\n",
            "547/547 [==============================] - 367s 670ms/step - loss: 0.1671 - accuracy: 0.9312 - val_loss: 0.1541 - val_accuracy: 0.9383\n",
            "Epoch 10/16\n",
            "547/547 [==============================] - 367s 671ms/step - loss: 0.1635 - accuracy: 0.9346 - val_loss: 0.1521 - val_accuracy: 0.9383\n",
            "Epoch 11/16\n",
            "547/547 [==============================] - 367s 670ms/step - loss: 0.1627 - accuracy: 0.9364 - val_loss: 0.1496 - val_accuracy: 0.9389\n",
            "Epoch 12/16\n",
            "547/547 [==============================] - 367s 671ms/step - loss: 0.1547 - accuracy: 0.9405 - val_loss: 0.1591 - val_accuracy: 0.9320\n",
            "Epoch 13/16\n",
            "547/547 [==============================] - 365s 668ms/step - loss: 0.1531 - accuracy: 0.9383 - val_loss: 0.1448 - val_accuracy: 0.9400\n",
            "Epoch 14/16\n",
            "547/547 [==============================] - 365s 667ms/step - loss: 0.1485 - accuracy: 0.9374 - val_loss: 0.1434 - val_accuracy: 0.9405\n",
            "Epoch 15/16\n",
            "547/547 [==============================] - 366s 668ms/step - loss: 0.1442 - accuracy: 0.9427 - val_loss: 0.1567 - val_accuracy: 0.9352\n",
            "Epoch 16/16\n",
            "547/547 [==============================] - 367s 671ms/step - loss: 0.1486 - accuracy: 0.9421 - val_loss: 0.1442 - val_accuracy: 0.9408\n",
            "time taken  5882.975666046143\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tOsIBkruvXig"
      },
      "source": [
        "transfer_learning('DenseNet169')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LQBx-iO0Ibjr"
      },
      "source": [
        "transfer_learning('vgg19')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jk9k3rjcIkpp"
      },
      "source": [
        "transfer_learning('InceptionV3')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lol2dij_9Kit"
      },
      "source": [
        "# running on TPU\n",
        "\n",
        "# !pip install keras==2.1\n",
        "# import tensorflow as tf\n",
        "# from tensorflow.keras.optimizers import SGD\n",
        "# from tensorflow.keras.models import Sequential, Model\n",
        "# from tensorflow.keras.layers import Conv2D, MaxPooling2D, Activation, Dropout, Flatten, Dense\n",
        "\n",
        "\n",
        "BASE_MODEL = 'VGG16'\n",
        "\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "if BASE_MODEL=='VGG16':\n",
        "    from keras.applications.vgg16 import VGG16 as PTModel, preprocess_input\n",
        "elif BASE_MODEL=='vgg19':\n",
        "    from keras.applications.vgg19 import VGG19 as PTModel, preprocess_input\n",
        "elif BASE_MODEL=='ResNet50':\n",
        "    from keras.applications.resnet50 import ResNet50 as PTModel, preprocess_input\n",
        "elif BASE_MODEL=='InceptionV3':\n",
        "    from keras.applications.inception_v3 import InceptionV3 as PTModel, preprocess_input\n",
        "elif BASE_MODEL=='Xception':\n",
        "    from keras.applications.xception import Xception as PTModel, preprocess_input\n",
        "elif BASE_MODEL=='DenseNet169': \n",
        "    from keras.applications.densenet import DenseNet169 as PTModel, preprocess_input\n",
        "elif BASE_MODEL=='DenseNet121':\n",
        "    from keras.applications.densenet import DenseNet121 as PTModel, preprocess_input\n",
        "else:\n",
        "    raise ValueError('Unknown model: {}'.format(BASE_MODEL))\n",
        "    \n",
        "    \n",
        "import keras\n",
        "keras.backend.set_learning_phase(1)\n",
        "    \n",
        "train_datagen = ImageDataGenerator(\n",
        "        rescale=1./255,\n",
        "        shear_range=0.2,\n",
        "        zoom_range=0.2,\n",
        "        horizontal_flip=True,\n",
        "        preprocessing_function = preprocess_input)\n",
        "\n",
        "val_datagen = ImageDataGenerator(rescale=1./255,\n",
        "                                preprocessing_function = preprocess_input)\n",
        "\n",
        "batch_size=32\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "        '/content/cats_dogs/data/train',  # this is the target directory\n",
        "        target_size=(200, 200),  # all images will be resized to 150x150\n",
        "        batch_size=batch_size,\n",
        "        class_mode='binary') \n",
        "\n",
        "validation_generator = val_datagen.flow_from_directory(\n",
        "        '/content/cats_dogs/data/val',\n",
        "        target_size=(200, 200),\n",
        "        batch_size=batch_size,\n",
        "        class_mode='binary')    \n",
        "\n",
        "img_rows, img_cols, img_channel = 200, 200, 3\n",
        "base_model = PTModel(weights='imagenet'\n",
        "                   ,include_top=False, input_shape=(img_rows, img_cols, img_channel), classes = 2)\n",
        "\n",
        "add_model = Sequential()\n",
        "add_model.add(Flatten(input_shape=base_model.output_shape[1:]))\n",
        "add_model.add(Dense(64, activation='relu'))\n",
        "add_model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "model = Model(inputs=base_model.input, outputs=add_model(base_model.output))\n",
        "\n",
        "\n",
        "for layer in base_model.layers:\n",
        "    layer.trainable = False\n",
        "    \n",
        "    if layer.name.startswith('bn'):\n",
        "        layer.call(layer.input, training=False)\n",
        "    \n",
        "    \n",
        "tpu_model = tf.contrib.tpu.keras_to_tpu_model(\n",
        "    model,\n",
        "    strategy=tf.contrib.tpu.TPUDistributionStrategy(\n",
        "        tf.contrib.cluster_resolver.TPUClusterResolver(tpu='grpc://' + os.environ['COLAB_TPU_ADDR'])\n",
        "    )\n",
        ")\n",
        "\n",
        "tpu_model.compile(loss='binary_crossentropy'\n",
        "              ,optimizer=SGD(lr=1e-4, momentum=0.9),\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "start_time = time()\n",
        "model.fit_generator(\n",
        "        train_generator,\n",
        "        epochs=16,\n",
        "        validation_data=validation_generator\n",
        "        #class_weight = class_weights,\n",
        "        ,callbacks=[ModelCheckpoint('Xception-transferlearning.model', monitor='val_acc', save_best_only=True)]\n",
        "        )\n",
        "model.save_weights('Xception.h5')\n",
        "\n",
        "print('time taken ',time()-start_time)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IwhhWk2j8cYe"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}