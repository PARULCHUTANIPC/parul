{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "fmnist d3",
      "provenance": [],
      "authorship_tag": "ABX9TyMbMvtxkPM4T/M++g8ozGGb",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/PARULCHUTANIPC/parul/blob/p1/fmnist_d3%2C4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zO_IsloSZPe5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "torch.set_printoptions(linewidth= 120)\n",
        "torch.set_grad_enabled(True)\n",
        "from torch.utils.tensorboard import SummaryWriter"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h_i2THEsZbTx",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "1a7ec897-25db-4317-f1e8-6c3625f5ddb3"
      },
      "source": [
        "\n",
        "def get_num_correct(preds, labels):\n",
        "  return preds.argmax(dim=1).eq(labels).sum().item()"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "ERROR! Session/line number was not unique in database. History logging moved to new session 60\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-0hc5_D6fZ31",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Network(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=6, kernel_size=5)\n",
        "        self.conv2 = nn.Conv2d(in_channels=6, out_channels=12, kernel_size=5)\n",
        "        \n",
        "        self.fc1 = nn.Linear(in_features=12 * 4 * 4, out_features=120)\n",
        "        self.fc2 = nn.Linear(in_features=120, out_features=60)\n",
        "        self.out = nn.Linear(in_features=60, out_features=10)\n",
        "        \n",
        "    def forward(self, t):\n",
        "        # implement the forward pass\n",
        "        # 1) Input layer\n",
        "        t=t\n",
        "\n",
        "        # (2) hidden conv layer\n",
        "        t = self.conv1(t)\n",
        "        t = F.relu(t)\n",
        "        t = F.max_pool2d(t, kernel_size=2, stride=2)\n",
        "\n",
        "        # (3) hidden conv layer\n",
        "        t = self.conv2(t)\n",
        "        t = F.relu(t)\n",
        "        t = F.max_pool2d(t, kernel_size=2, stride=2)\n",
        "\n",
        "        # (4) hidden linear layer\n",
        "        t = t.reshape(-1, 12 * 4 * 4)\n",
        "        t = self.fc1(t)\n",
        "        t = F.relu(t)\n",
        "\n",
        "        # (5) hidden linear layer\n",
        "        t = self.fc2(t)\n",
        "        t = F.relu(t)\n",
        "\n",
        "        # 6) Output Layer\n",
        "        t = self.out(t)\n",
        "#t = F.softmax(t, dim=1)\n",
        "\n",
        "        return t"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RbR9afE7uPux",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xYaEs6uifiFz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_set = torchvision.datasets.FashionMNIST(\n",
        "    root='./data'\n",
        "    ,train=True\n",
        "    ,download=True\n",
        "    ,transform=transforms.Compose([\n",
        "        transforms.ToTensor()\n",
        "    ])\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vlYmxc9MfvOV",
        "colab_type": "text"
      },
      "source": [
        "# TRAINING WITH A SINGLE BATCH"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wPH-Rmdifs6U",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "network = Network()\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(train_set\n",
        "   ,batch_size=100)\n",
        "\n",
        "optimizer = optim.Adam(network.parameters(),lr = 0.01)\n",
        "\n",
        "# Obtain the batch \n",
        "batch = next(iter(train_loader))\n",
        "images, labels = batch\n",
        "\n",
        "\n",
        "# PASS THE BATCH IN THE NETWORK  --------  network is the name of the class we made..... in which we defined the layers, 1 input, 2 conv, 2 linear and 1 output layer\n",
        "preds = network(images)\n",
        "\n",
        "#CALCULATE LOSS\n",
        "loss = F.cross_entropy(preds, labels)\n",
        "\n",
        "# CALCULATE GRADIENTS\n",
        "loss.backward()\n",
        "\n",
        "# UPDATE WEIGHTS\n",
        "optimizer.step()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-QhPTrHT9PPJ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "5367b563-377b-4363-a4d7-0119afa57240"
      },
      "source": [
        "print ('loss1 : ', loss.item()) # it will give the loss when the images are given to the network  first time\n",
        "# Now, the images are again passed to the network and calculate the loss again and compare\n",
        "preds = network(images)\n",
        "loss = F.cross_entropy(preds, labels)\n",
        "print('loss2 : ', loss.item())\n"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "loss1 :  2.2978203296661377\n",
            "loss2 :  2.2623584270477295\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l64PwpVbAm7S",
        "colab_type": "text"
      },
      "source": [
        "**# TRAINING WITH ALL BATCHES / ONE EPOCH**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MwpzY2pr_xh5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "network = Network()\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(train_set\n",
        "   ,batch_size=100)\n",
        "\n",
        "optimizer = optim.Adam(network.parameters(),lr = 0.01)\n",
        "\n",
        "# THIS IS THE SAME AS TRAINING A SINGLE BATCH. EXCEPT TO THAT , WE HAVE TO APPLY A LOOP SO THAT ALL BATCHES CAN PASS THROUGH THE NETWORK TO GET TRAINED\n",
        "#####---------------------------\n",
        "total_loss= 0\n",
        "total_correct = 0\n",
        "\n",
        "\n",
        "# Obtain the batch \n",
        "#batch = next(iter(train_loader))\n",
        "#images, labels = batch\n",
        "\n",
        "# SELECTING ALL BATCHES\n",
        "\n",
        "for batch in train_loader:\n",
        "\n",
        "# PASS THE BATCH IN THE NETWORK  --------  network is the name of the class we made..... in which we defined the layers, 1 input, 2 conv, 2 linear and 1 output layer\n",
        "  preds = network(images)\n",
        "\n",
        "#CALCULATE LOSS\n",
        "  loss = F.cross_entropy(preds, labels)\n",
        "\n",
        "\n",
        "# -------------------- EXTRA LINE ---- SET THE INITIAL GRADIENT ZERO AFTER EVERY ITERATION, OTHERWISE IT WILL ADD UP THE WEIGHT AFTER EVERY STEP WHICH WILL ULTIMATELY RESULT IN BAD ACCURACY, HIGHER LOSS\n",
        "\n",
        "  optimizer.zero_grad()\n",
        "\n",
        "# CALCULATE GRADIENTS\n",
        "  loss.backward()\n",
        "\n",
        "# UPDATE WEIGHTS  \n",
        "  optimizer.step()\n",
        "\n",
        "  ##----------- Now, add the loss calculated  and the correct predictions done for every batches\n",
        "  total_correct+= get_num_correct(preds, labels)\n",
        "  total_loss+= loss.item()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ojs1_WcgBtbG",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "967e7da7-d931-4f57-b5ac-54e7cdea5037"
      },
      "source": [
        "print(\"epoch : \", 0, \"Correct Predictions : \", total_correct, \"   Total_Loss : \", total_loss)\n",
        "accuracy = total_correct/len(train_set)\n",
        "print (\"Accuracy : \", accuracy)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "epoch :  0 Correct Predictions :  58477    Total_Loss :  40.48264418143481\n",
            "Accuracy :  0.9746166666666667\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Ln3uaO_Dl7W",
        "colab_type": "text"
      },
      "source": [
        "# TRAIN THE IMAGES MULTIPLE TIMES / multiple epochs"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jf4JIRRCC9y7",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 130
        },
        "outputId": "42b185ab-a9c4-4f91-f4ef-2040eef41aa7"
      },
      "source": [
        "network = Network()\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(train_set\n",
        "   ,batch_size=100)\n",
        "\n",
        "optimizer = optim.Adam(network.parameters(),lr = 0.01)\n",
        "\n",
        "# Obtain the batch \n",
        "batch = next(iter(train_loader))\n",
        "images, labels = batch\n",
        "\n",
        "################ ADDING THE TENSORBOARD  ########&&&&&&&&&&&&&&&&&&&&&\n",
        "grid = torchvision.utils.make_grid(images)\n",
        "\n",
        "tb= SummaryWriter()\n",
        "tb.add_image('images', grid)\n",
        "tb.add_graph(network, images)\n",
        "\n",
        "######################            CREATE A LOOOPPPPPPP ##########################################\n",
        "\n",
        "\n",
        "for epoch in range(3):\n",
        "\n",
        "\n",
        "# THIS IS THE SAME AS TRAINING A SINGLE BATCH. EXCEPT TO THAT , WE HAVE TO APPLY A LOOP SO THAT ALL BATCHES CAN PASS THROUGH THE NETWORK TO GET TRAINED\n",
        "#####---------------------------\n",
        "  total_loss= 0\n",
        "  total_correct = 0\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# SELECTING ALL BATCHES\n",
        "\n",
        "  for batch in train_loader:\n",
        "\n",
        "# PASS THE BATCH IN THE NETWORK  --------  network is the name of the class we made..... in which we defined the layers, 1 input, 2 conv, 2 linear and 1 output layer\n",
        "    preds = network(images)\n",
        "\n",
        "#CALCULATE LOSS\n",
        "    loss = F.cross_entropy(preds, labels)\n",
        "\n",
        "\n",
        "# -------------------- EXTRA LINE ---- SET THE INITIAL GRADIENT ZERO AFTER EVERY ITERATION, OTHERWISE IT WILL ADD UP THE WEIGHT AFTER EVERY STEP WHICH WILL ULTIMATELY RESULT IN BAD ACCURACY, HIGHER LOSS\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "# CALCULATE GRADIENTS\n",
        "    loss.backward()\n",
        "\n",
        "# UPDATE WEIGHTS  \n",
        "    optimizer.step()\n",
        "\n",
        "  ##----------- Now, add the loss calculated  and the correct predictions done for every batches\n",
        "    total_correct+= get_num_correct(preds, labels)\n",
        "    total_loss+= loss.item()\n",
        "\n",
        "###-------------------ADDING THE TENSORBOARD--------------------------------\n",
        "  tb.add_scalar('Loss', total_loss, epoch)\n",
        "  tb.add_scalar('Correct Predictions', total_correct, epoch)\n",
        "  tb.add_scalar('Accuracy', total_correct/len(train_set), epoch)\n",
        "  tb.add_histogram('Conv1.bias', network.conv1.bias, epoch)\n",
        "  tb.add_histogram('Conv1.weight', network.conv1.weight, epoch)\n",
        "  tb.add_histogram('Conv1.weight.grad', network.conv1.weight.grad, epoch)\n",
        "\n",
        "\n",
        "  print(\"epoch : \", epoch, \"Correct Predictions : \", total_correct, \"   Total_Loss : \", total_loss)\n",
        "tb.close()"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "ERROR! Session/line number was not unique in database. History logging moved to new session 61\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will switch to TensorFlow 2.x on the 27th of March, 2020.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now\n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "epoch :  0 Correct Predictions :  58474    Total_Loss :  39.346368390321004\n",
            "epoch :  1 Correct Predictions :  60000    Total_Loss :  0.0062781407582406246\n",
            "epoch :  2 Correct Predictions :  60000    Total_Loss :  0.002183776653737368\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NYG-ffaVOKFv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#print(\"epoch : \", epoch, \"Correct Predictions : \", total_correct, \"   Total_Loss : \", total_loss)\n",
        "accuracy = total_correct/len(train_set)\n",
        "print (\"Accuracy : \", accuracy*100,\"%\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "511aXw9Nb-xK",
        "colab_type": "text"
      },
      "source": [
        "#ANALYZING CNN RESULTS / CONFUSION MATRIX"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aZ_CVpJZPJAT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(len(train_set))\n",
        "len(train_set.targets)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3jfmSdU4cO64",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import numpy as np\n",
        "#from resources.plotcm import plot_confusion_matrix"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CyROJwGIAmk5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bZqzK0ZFAmAu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "network = Network()\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(train_set\n",
        "   ,batch_size=100)\n",
        "\n",
        "optimizer = optim.Adam(network.parameters(),lr = 0.01)\n",
        "\n",
        "\n",
        "\n",
        "######################            CREATE A LOOOPPPPPPP ##########################################\n",
        "\n",
        "\n",
        "for epoch in range(5):\n",
        "\n",
        "\n",
        "# THIS IS THE SAME AS TRAINING A SINGLE BATCH. EXCEPT TO THAT , WE HAVE TO APPLY A LOOP SO THAT ALL BATCHES CAN PASS THROUGH THE NETWORK TO GET TRAINED\n",
        "#####---------------------------\n",
        "  total_loss= 0\n",
        "  total_correct = 0\n",
        "  total_pred = []\n",
        "  \n",
        "# Obtain the batch \n",
        "#batch = next(iter(train_loader))\n",
        "#images, labels = batch\n",
        "\n",
        "# SELECTING ALL BATCHES\n",
        "\n",
        "  for batch in train_loader:\n",
        "\n",
        "# PASS THE BATCH IN THE NETWORK  --------  network is the name of the class we made..... in which we defined the layers, 1 input, 2 conv, 2 linear and 1 output layer\n",
        "    preds = network(images)\n",
        "\n",
        "#CALCULATE LOSS\n",
        "    loss = F.cross_entropy(preds, labels)\n",
        "\n",
        "\n",
        "# -------------------- EXTRA LINE ---- SET THE INITIAL GRADIENT ZERO AFTER EVERY ITERATION, OTHERWISE IT WILL ADD UP THE WEIGHT AFTER EVERY STEP WHICH WILL ULTIMATELY RESULT IN BAD ACCURACY, HIGHER LOSS\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "# CALCULATE GRADIENTS\n",
        "    loss.backward()\n",
        "\n",
        "# UPDATE WEIGHTS  \n",
        "    optimizer.step()\n",
        "\n",
        "  ##----------- Now, add the loss calculated  and the correct predictions done for every batches\n",
        "    total_correct+= get_num_correct(preds, labels)\n",
        "    total_loss+= loss.item()\n",
        "    total_pred.append(preds)\n",
        "    \n",
        "  print(\"epoch : \", epoch, \"Correct Predictions : \", total_correct, \"   Total_Loss : \", total_loss)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JKyZ2IzbEZKY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "len(total_pred)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kKDkD7SJ-2o6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "############################## Code by Soumith#######################\n",
        "prediction_list=[]\n",
        "def predict(self, dataloader):\n",
        "  for i, batch in enumerate(dataloader):\n",
        "    pred,output  = self.step(batch)\n",
        "    prediction_list.append(pred.cpu())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5HPcu5z5_j6D",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "predict()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gaN54sMUEio3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#type(total_pred)\n",
        "len(train_set.targets)\n",
        "train_preds"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RAYQ1cm0XKWy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(len(total_pred[1]))\n",
        "print(len(total_pred[1][1]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sXnBlv1GAT1v",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fmQdRKn_HPHk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "listt = []\n",
        "for i in total_pred:\n",
        "  for j in i:\n",
        "    listt.append(j.argmax(dim=0).tolist())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SRu-FLKHkw21",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "total_pred[1].argmax(dim=0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oPGau0wrHO3Z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(len(listt))\n",
        "print(type(listt))\n",
        "type(listt[0])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n-a3JHNWTkbu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "labels_to_compare = []\n",
        "for i in train_set.train_labels:\n",
        "  labels_to_compare.append(i.tolist())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3TCNPG7FUWhd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(type(labels_to_compare))\n",
        "print(type(labels_to_compare[0]))\n",
        "len(labels_to_compare)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L1eGoAS-kHoP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "cm = confusion_matrix(labels_to_compare, listt)\n",
        "#print(type(cm))\n",
        "#cm.dtype\n",
        "cm"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f7RphAG0a5VP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "labels_to_compare == listt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xzMRpzMib7wU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "not_same = 0\n",
        "for i in range(len(listt)):\n",
        "  if listt[i] != labels_to_compare[i]:\n",
        "    not_same+= 1\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wmibec0HcMO3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "not_same"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kn6aOVFadfzT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "s1=[1,1,1]\n",
        "s2=[1,1,1]\n",
        "s1==s2"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G4ZU3CkbctxP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.metrics import confusion_matrix"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QYnhlEMndNXq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "cm = confusion_matrix(labels_to_compare, listt)\n",
        "cm"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RmNIIb5HkWeC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sns.heatmap(data = cm)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PDbxyK9Ml2Kp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        " names = (\n",
        "    'T-shirt/top'\n",
        "    ,'Trouser'\n",
        "    ,'Pullover'\n",
        "    ,'Dress'\n",
        "    ,'Coat'\n",
        "    ,'Sandal'\n",
        "    ,'Shirt'\n",
        "    ,'Sneaker'\n",
        "    ,'Bag'\n",
        "    ,'Ankle boot'\n",
        ")\n",
        "plt.figure(figsize=(10,10))\n",
        "#plt.plot(data = cm)\n",
        "\n",
        "#plot_confusion_matrix(cm, names)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d7v2WgGbaHxR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.metrics import classification_report"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r8AllwQsa12i",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P3ZAa-aiyc-k",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(classification_report(labels_to_compare, listt))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yKKBJEFPyu5a",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s-zaW2Ubz_bD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for i in range(len(listt)):\n",
        "  labels_to_compare[i]==listt[i]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_ZfoJTuwNxIN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "emdLInUVN1qQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1s0zMpcAOB_S",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "with torch.no_grad():\n",
        "    prediction_loader = torch.utils.data.DataLoader(train_set, batch_size=10000)\n",
        "    train_preds = get_all_preds(network, prediction_loader)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6ahcQgDIo93N",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}